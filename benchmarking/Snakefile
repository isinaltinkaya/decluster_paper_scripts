import os
import pandas as pd


WDIR="/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/"
SAMPLE,=glob_wildcards(WDIR+"data/bam/{sample}-GRCh37.bam")

dataset={}
for s in SAMPLE:
	dataset[s]={"BAM": [WDIR+"data/bam/"+s+"-hg38.bam"],
			"FQ": [WDIR+"data/fq/"+s+".R1.fq.gz", WDIR+"data/fq/"+s+".R2.fq.gz"]}


	DIST=["global","local"]

REF="/maps/projects/lundbeck/data/hg38/genome.fa"


rule all:
	input:
		# expand(WDIR+"results/adapter_removal/{sample}.collapsed.gz",
				# sample=SAMPLE),
		# expand(WDIR+"results/bwa_aln/{sample}.collapsed.sai",
				# sample=SAMPLE),
		# expand(WDIR+"results/bwa_samse/{sample}.bam",
				# sample=SAMPLE),
		# expand(WDIR+"results/validate_fq/{sample}.validate.out",
				# sample=SAMPLE),
		# expand("results/clumpify/collapsed_fq/{sample}.fastq",
				# sample=SAMPLE),
		# expand("results/clumpify/collapsed_fq/bwa_aln/{sample}.sai",
				# sample=SAMPLE),
		# expand("results/clumpify/collapsed_fq/bwa_samse/{sample}_clumpify-dedup.bam",
				# sample=SAMPLE),
		# expand("results/clumpify/collapsed_fq/preseq/{sample}_clumpify-dedup-preseq.table.txt",
				# sample=SAMPLE),
		# expand("results/decluster/collapsed_bam/preseq/{distmode}/{sample}_collapsed-decluster-preseq.table.txt",
				# distmode=DIST,
				# sample=SAMPLE),
		# expand("results/decluster/collapsed_bam/{distmode}/{sample}.out.noClusterDuplicates.bam",
				# distmode=DIST,
				# sample=SAMPLE),
		# "results/benchmark/merged_benchmark",
		###
		###
		###
		# expand("results/clumpify/collapsed_fq/t1/{sample}.fastq",
				# sample=SAMPLE),
		# expand(WDIR+"results/bwa_samse/F4/{sample}_F4.bam",
				# sample=SAMPLE),
		expand("results/decluster/collapsed_bam/{distmode}/F4/{sample}.out.noClusterDuplicates.bam",
				distmode=DIST,
				sample=SAMPLE),
		expand("results/decluster/collapsed_bam/preseq/{distmode}/F4/{sample}_collapsed-decluster-preseq.table.txt",
				distmode=DIST,
				sample=SAMPLE),
		"results/benchmark/merged_benchmark_v2"


rule validate_fq:
	input:
		R1=lambda wildcards: dataset[wildcards.sample]["FQ"][0],
		R2=lambda wildcards: dataset[wildcards.sample]["FQ"][1]
	output:
		WDIR+"results/validate_fq/{sample}.validate.out"
	shell:
		"""
		java -jar /maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/tools/validatefastq-assembly-0.1.1.jar -i {input.R1} -j {input.R2} > {output} 2>&1
		"""



rule adapter_removal:
	input:
		R1=lambda wildcards: dataset[wildcards.sample]["FQ"][0],
		R2=lambda wildcards: dataset[wildcards.sample]["FQ"][1]
	output:
		WDIR+"results/adapter_removal/{sample}.collapsed.gz"
	params:
		AdapterRemoval="/maps/projects/lundbeck/apps/adapterremoval-2.3.2/build/AdapterRemoval",
		prefix=WDIR+"results/adapter_removal/{sample}",
		# prefix="{sample}",
	log:
		WDIR+"results/logs/adapter_removal/{sample}.collapsed.gz"
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		( {params.AdapterRemoval} --file1 {input.R1} --file2 {input.R2} --basename {params.prefix}	--minlength 30 --collapse-conservatively --gzip --adapter1 "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA" --adapter2 "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT") 2> {log}
		"""



rule bwa_aln:
	input:
		collapsed_gz=WDIR+"results/adapter_removal/{sample}.collapsed.gz",
		# pair1_truncated_gz=WDIR+"results/adapter_removal/{sample}.pair1.truncated.gz",
		# pair2_truncated_gz=WDIR+"results/adapter_removal/{sample}.pair2.truncated.gz",
	output:
		collapsed_sai=WDIR+"results/bwa_aln/{sample}.collapsed.sai",
		# pair1_sai=WDIR+"results/bwa_aln/{sample}.pair1.sai",
		# pair2_sai=WDIR+"results/bwa_aln/{sample}.pair2.sai",
	params:
		ref=REF,
	log:
		WDIR+"results/logs/bwa_aln/{sample}.collapsed.sai",
	threads: 8
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		( bwa aln -t {threads} {params.ref} {input.collapsed_gz} -l 32 -f {output.collapsed_sai} ) 2> {log}
		"""

		# ( bwa aln -t {threads} {params.ref} {input.pair1_truncated_gz} -l 32 -f {output.pair1_sai} ) 2> {log}.p1
		# ( bwa aln -t {threads} {params.ref} {input.pair2_truncated_gz} -l 32 -f {output.pair2_sai} ) 2> {log}.p2

#get collapsed output
# rule bwa_samse:
	# input:
		# collapsed_gz=WDIR+"results/adapter_removal/{sample}.collapsed.gz",
		# collapsed_sai=WDIR+"results/bwa_aln/{sample}.collapsed.sai",
	# output:
		# WDIR+"results/bwa_samse/{sample}.bam",
	# params:
		# ref=REF,
		# readgroup=r'@RG\tID:{sample}\tSM:{sample}\tLB:collapsed',
	# threads:
		# 8
	# log:
		# WDIR+"results/logs/bwa_samse/{sample}.bam",
	# conda:
		# "/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	# shell:
		# """
		# ( bwa samse -r '{params.readgroup}' {params.ref} {input.collapsed_sai} {input.collapsed_gz} | samtools sort -@{threads} -o {output} ) 2> {log}
		# """
#
#
# #get paired output
# rule bwa_sampe:
	# input:
		# pair1_sai=WDIR+"results/bwa_aln/{sample}.pair1.sai",
		# pair2_sai=WDIR+"results/bwa_aln/{sample}.pair2.sai",
		# pair1_truncated_gz=WDIR+"results/adapter_removal/{sample}.pair1.truncated.gz",
		# pair2_truncated_gz=WDIR+"results/adapter_removal/{sample}.pair2.truncated.gz",
	# output:
		# WDIR+"results/bwa_sampe/{sample}.bam",
	# params:
		# ref=REF,
		# readgroup=r'@RG\tID:{sample}\tSM:{sample}\tLB:paired',
	# log:
		# WDIR+"results/logs/bwa_sampe/{sample}.bam",
	# conda:
		# "/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	# shell:
		# """
		# ( bwa sampe -r '{params.readgroup}' {params.ref} {input.pair1_sai} {input.pair2_sai} {input.pair1_truncated_gz} {input.pair1_truncated_gz} | samtools sort -@{threads} -o {output} ) 2> {log}
		# """
#


#
# Benchmark 1)
# Run clumpify on the "collapsed fastq", benchmark time
# Map the output fastq from clumpify that contains the pcr dups (therefore cluster duplicates removed)
# Run preseq
#
# rule run_clumpify_collapsed:
	# input:
		# collapsed_gz=WDIR+"results/adapter_removal/{sample}.collapsed.gz",
	# output:
		# "results/clumpify/collapsed_fq/{sample}.fastq"
	# params:
		# clumpify="/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/tools/bbmap/clumpify.sh",
	# log:
		# "results/logs/clumpify/collapsed_fq/{sample}.out"
	# benchmark:
		# repeat("results/benchmark/clumpify/collapsed_fq/{sample}.out",20)
	# shell:
		# """
		# ( bash {params.clumpify} -in={input} -out={output} -dupedist=20000 dedupe=t optical=t dupedist=20000 spantiles=t adjacent=t ) 2> {log}
		# """
#

rule run_clumpify_collapsed_disableThreads:
	input:
		collapsed_gz=WDIR+"results/adapter_removal/{sample}.collapsed.gz",
	output:
		"results/clumpify/collapsed_fq/t1/{sample}.fastq"
	params:
		clumpify="/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/tools/bbmap/clumpify.sh",
	log:
		"results/logs/clumpify/collapsed_fq/t1/{sample}.out"
	benchmark:
		repeat("results/benchmark/clumpify/collapsed_fq/t1/{sample}.out",20)
	shell:
		"""
		( bash {params.clumpify} threads=1 -in={input} -out={output} -dupedist=20000 dedupe=t optical=t dupedist=20000 spantiles=t adjacent=t ) 2> {log}
		"""



rule bwa_aln_dedup_collapsed:
	input:
		"results/clumpify/collapsed_fq/{sample}.fastq"
	output:
		"results/clumpify/collapsed_fq/bwa_aln/{sample}.sai"
	params:
		ref=REF,
	log:
		"results/logs/clumpify/collapsed_fq/bwa_aln/{sample}.sai"
	threads: 8
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		( bwa aln -t {threads} {params.ref} {input} -l 32 -f {output} ) 2> {log}
		"""


rule bwa_samse_dedup_collapsed:
	input:
		fq="results/clumpify/collapsed_fq/{sample}.fastq",
		sai="results/clumpify/collapsed_fq/bwa_aln/{sample}.sai"
	output:
		"results/clumpify/collapsed_fq/bwa_samse/{sample}_clumpify-dedup.bam"
	params:
		ref=REF,
		readgroup=r'@RG\tID:{sample}\tSM:{sample}\tLB:collapsed_clumpify-dedup',
	threads:
		8
	log:
		"results/logs/clumpify/collapsed_fq/bwa_samse/{sample}_clumpify-dedup.bam"
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		( bwa samse -r '{params.readgroup}' {params.ref} {input.sai} {input.fq} | samtools sort -@{threads} -o {output} ) 2> {log}
		"""


rule run_preseq_after_clumpify:
	input:
		"results/clumpify/collapsed_fq/bwa_samse/{sample}_clumpify-dedup.bam"
	output:
		"results/clumpify/collapsed_fq/preseq/{sample}_clumpify-dedup-preseq.table.txt",
	log:
		"results/logs/clumpify/collapsed_fq/preseq/{sample}_clumpify-dedup-preseq.table.txt",
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		(
		preseq lc_extrap -B {input} -o {output}
		) 2> {log}
		"""



# Benchmark 2)
# Map the "collapsed fastq"
# Run decluster with bams created in the previous step, benchmark time
# Run preseq
#

rule remove_unmapped_F4:
	input:
		WDIR+"results/bwa_samse/{sample}.bam",
	output:
		WDIR+"results/bwa_samse/F4/{sample}_F4.bam",
	log:
		WDIR+"results/logs/bwa_samse/F4/{sample}_F4.bam",
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		( samtools view -bh -F 4 {input} > {output} )2> {log}
		"""

# rule run_decluster_distmode:
	# input:
		# WDIR+"results/bwa_samse/{sample}.bam",
	# output:
		# "results/decluster/collapsed_bam/{distmode}/{sample}.out.noClusterDuplicates.bam"
	# params:
		# decluster="/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/tools/decluster/decluster",
		# prefix="results/decluster/collapsed_bam/{distmode}/{sample}.out"
	# log:
		# "results/logs/decluster/{distmode}/{sample}.out"
	# benchmark:
		# repeat("results/benchmark/decluster/{distmode}/{sample}.out", 20)
	# shell:
		# """
		# ({params.decluster} -0 -d {wildcards.distmode} -b -p 20000 -@ 1 -q 30 -o {params.prefix} {input}) 2> {log}
		# """
#

rule run_decluster_distmode_F4:
	input:
		WDIR+"results/bwa_samse/F4/{sample}_F4.bam",
	output:
		"results/decluster/collapsed_bam/{distmode}/F4/{sample}.out.noClusterDuplicates.bam"
	params:
		decluster="/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/tools/decluster/decluster",
		prefix="results/decluster/collapsed_bam/{distmode}/F4/{sample}.out"
	log:
		"results/logs/decluster/{distmode}/F4/{sample}.out"
	benchmark:
		repeat("results/benchmark/decluster/{distmode}/F4/{sample}.out", 20)
	shell:
		"""
		({params.decluster} -0 -d {wildcards.distmode} -b -p 20000 -@ 1 -q 30 -o {params.prefix} {input}) 2> {log}
		"""



#
# rule run_preseq_after_decluster:
	# input:
		# "results/decluster/collapsed_bam/{distmode}/{sample}.out.noClusterDuplicates.bam"
	# output:
		# "results/decluster/collapsed_bam/preseq/{distmode}/{sample}_collapsed-decluster-preseq.table.txt"
	# log:
		# "results/logs/decluster/collapsed_bam/preseq/{distmode}/{sample}_collapsed-decluster-preseq.table.txt"
	# conda:
		# "/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	# shell:
		# """
		# (
		# preseq lc_extrap -B {input} -o {output}
		# ) 2> {log}
		# """



rule run_preseq_after_decluster_F4:
	input:
		"results/decluster/collapsed_bam/{distmode}/F4/{sample}.out.noClusterDuplicates.bam"
	output:
		"results/decluster/collapsed_bam/preseq/{distmode}/F4/{sample}_collapsed-decluster-preseq.table.txt"
	log:
		"results/logs/decluster/collapsed_bam/preseq/{distmode}/F4/{sample}_collapsed-decluster-preseq.table.txt"
	conda:
		"/maps/projects/lundbeck/scratch/pfs488/decluster/benchmark_2211/decluster_paper_scripts/env/decluster_benchmarking_env.yml"
	shell:
		"""
		(
		preseq lc_extrap -B {input} -o {output}
		) 2> {log}
		"""





# "results/benchmark/decluster/{distmode}/{sample}.out"
# "results/benchmark/clumpify/collapsed_fq/{sample}.out"

rule collect_benchmark_results:
	output:
		"results/benchmark/merged_benchmark"
	run:

		bmd = glob_wildcards("results/benchmark/{tool}/{sample}.out")
		files = [ os.path.abspath("results/benchmark/" + tool + "/" + sample + ".out") for tool,sample in zip(bmd.tool,bmd.sample) ]
		dfs=[]
		for f, tool, sample in zip(files, bmd.tool,bmd.sample):
			fr=pd.read_csv(f,sep='\t')
			fr['sample']=sample
			fr['tool'] = tool.replace("/","_")
			dfs.append(fr)

		all_dfs=pd.concat(dfs)

		with(open(output[0],'w')) as of:
			all_dfs.to_csv(of,index=False)


# v2: clumpify threads=1, decluster input F4
rule collect_benchmark_results_v2:
	output:
		"results/benchmark/merged_benchmark_v2"
	run:

		# target:
		# "results/logs/decluster/{distmode}/F4/{sample}.out"
		# "results/benchmark/clumpify/collapsed_fq/t1/{sample}.out"
		bmd = glob_wildcards("results/benchmark/{tool}/{conf1}/{conf2}/{sample}.out")
		# print(bmd)

		files = [ os.path.abspath("results/benchmark/" + tool + "/" + conf1 + "/" + conf2 + "/" + sample + ".out") for tool,conf1,conf2,sample in zip(bmd.tool,bmd.conf1,bmd.conf2,bmd.sample) ]
		dfs=[]
		for f, tool, conf1, conf2, sample in zip(files, bmd.tool,bmd.conf1,bmd.conf2,bmd.sample):
			fr=pd.read_csv(f,sep='\t')
			fr['sample']=sample
			fr['id'] = str(tool) + "_" + str(conf1) + "_" + str(conf2)
			fr['tool']= tool
			fr['conf'] = str(conf1) + "_" + str(conf2)
			dfs.append(fr)

		all_dfs=pd.concat(dfs)

		with(open(output[0],'w')) as of:
			all_dfs.to_csv(of,index=False)



rule plot_benchmark_results:
	input:
		expand("results/benchmark/decluster/{distmode}/{sample}.out", 
				distmode=DIST,
				sample=SAMPLE),
	output:
		"results/plot/decluster_benchmark.pdf"
	envmodules:
		["gcc","R/4.2.1"]
	shell:
		"""
		Rscript plot_benchmark.R {input} {output}
		"""



